{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-sheriff",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "cuda: 2\n",
    "'''\n",
    "import os\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"4\"\n",
    "import math\n",
    "from visdom import Visdom\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import pandas as pd\n",
    "from utils import *\n",
    "from gen_data_nomix import *\n",
    "from networks import GetFeature, D, TGCN\n",
    "from graph_part.metis_1 import graph_part, get_part_data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-michael",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(a,b):\n",
    "    rmse = math.sqrt(mean_squared_error(a,b))\n",
    "    mae = mean_absolute_error(a,b)\n",
    "    mape = mean_absolute_percentage_error(a,b)\n",
    "    F_norm = 1-mae/a.mean()\n",
    "    r2 = 1 - ((a-b)**2).sum()/((a-a.mean())**2).sum()\n",
    "    var = 1-(np.var(a-b))/np.var(a)\n",
    "    return rmse, mae, mape, F_norm, r2, var\n",
    "\n",
    "def tgcn_loss(y_pred, y_true):\n",
    "    lambda_loss = 0.0015\n",
    "    Lreg = 0 # 正则化项\n",
    "    for para in net.parameters():\n",
    "        Lreg += torch.sum(para ** 2) / 2\n",
    "    Lreg = lambda_loss * Lreg\n",
    "\n",
    "    regress_loss = torch.sum((y_pred-y_true) ** 2) / 2\n",
    "    return regress_loss + Lreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-works",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['CUDA_VISIBLE_DEVICES']='5'\n",
    "device = torch.device('cuda:2')\n",
    "cwd = os.getcwd()\n",
    "viz = Visdom()\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# torch.cuda.set_device(1)\n",
    "#===== para set: =====#\n",
    "epochs = 500\n",
    "load_batch_size = 32\n",
    "seq_len = 12\n",
    "pre_len = 6\n",
    "gru_units = 64\n",
    "src_city = 'bj'\n",
    "tgt_city = 'sh'\n",
    "dt_similar_lambda = 1\n",
    "sub_graph_num = 4\n",
    "min_mape = 100\n",
    "if_norm = 'zscore'\n",
    "inv_if_norm = 'inv_'+if_norm\n",
    "tgt_num_days = 10\n",
    "environ=str(pre_len*5)+'min_'+str(tgt_num_days)+'DAY_main_400_nomix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-ivory",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===== data load: =====# \n",
    "src_speed = CityDataset(city=src_city, typo='speed', seq_len=seq_len, pre_len=pre_len, num_days=tgt_num_days, if_norm=if_norm)\n",
    "src_loader = DataLoader(src_speed, batch_size=load_batch_size, pin_memory=True, num_workers=0, shuffle=True)\n",
    "tgt_speed = CityDataset(city=tgt_city, typo='speed', seq_len=seq_len, pre_len=pre_len, num_days=tgt_num_days, if_norm=if_norm)\n",
    "tgt_loader = DataLoader(tgt_speed, batch_size=load_batch_size, pin_memory=True, num_workers=0, shuffle=True)\n",
    "src_num_nodes, tgt_num_nodes = src_speed.data.shape[2], tgt_speed.data.shape[2]\n",
    "src_adj = pd.read_csv('./src_data/'+src_city+'_adj.csv', header=None).values\n",
    "tgt_adj = pd.read_csv('./src_data/'+tgt_city+'_adj.csv', header=None).values\n",
    "tgt_adj = tgt_adj[:400, :400]\n",
    "tgt_nodes_part, tgt_min_num = graph_part('sh',tgt_adj, sub_graph_num)\n",
    "src_nodes_part, src_min_num = graph_part('bj',src_adj, (src_num_nodes//tgt_min_num)-1)\n",
    "\n",
    "#===== model init: =====#\n",
    "net = TGCN(tgt_min_num, 1, gru_units, seq_len, pre_len).to(device=device)\n",
    "# net = net.double()\n",
    "net.train()\n",
    "set_requires_grad(net, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-discharge",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_speed.__len__(), tgt_speed.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-office",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if os.path.exists(cwd+'/pkl/NETS_params.pkl'):\n",
    "#     checkpoint = torch.load(cwd+'/pkl/NETS_params.pkl')\n",
    "#     TranFeatureNet.load_state_dict(checkpoint['TF_model'])\n",
    "#     discriminator.load_state_dict(checkpoint['D'])\n",
    "#     net.load_state_dict(checkpoint['net'])\n",
    "#     min_mape = checkpoint['min_mape']\n",
    "#     print('加载模型成功')\n",
    "\n",
    "#===== optim: =====# \n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=0.001)\n",
    "\n",
    "Train_loss = []\n",
    "Val_loss = []\n",
    "\n",
    "validation_maes = []\n",
    "validation_pred = []\n",
    "validation_mapes = []\n",
    "validation_acc = []\n",
    "validation_r2 = []\n",
    "validation_var = []\n",
    "validation_rmses = []\n",
    "\n",
    "\n",
    "viz.line([0.],[0.],env=environ, win='train_loss',opts=dict(title='train loss'))\n",
    "viz.line([0.],[0.],env=environ, win='val_loss',opts=dict(title='val loss'))\n",
    "viz.line([0.],[0.],env=environ, win='rmses',opts=dict(title='rmses'))\n",
    "viz.line([0.],[0.],env=environ, win='maes',opts=dict(title='maes'))\n",
    "viz.line([0.],[0.],env=environ, win='mapes',opts=dict(title='mapes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-phone",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    torch.manual_seed(3)\n",
    "#     batches = zip(src_loader)\n",
    "    total_batch_loss = 0\n",
    "    n_batches = len(src_loader)\n",
    "    total_domain_loss = total_label_accuracy = 0\n",
    "    total_similar_loss = 0\n",
    "    tgcn_tlt_loss = []\n",
    "    print('==========EPOCH %03d==========='%(epoch))\n",
    "    # print('==========EPOCH {0:0>3}==========='.format(epoch))\n",
    "\n",
    "##########################\n",
    "# mix_data混合数据，输出tgt_data_gen。\n",
    "##########################\n",
    "    for src_data, src_pre in tqdm(src_loader, leave=False, total=n_batches):\n",
    "        # ===== 数据转移到GPU\n",
    "        src_data = src_data.to(device)\n",
    "        if src_data.shape[0] != load_batch_size:\n",
    "            continue\n",
    "        \n",
    "\n",
    "#         src_data = torch.cat((src_data1, src_data2, src_data3),0)\n",
    "#         src_pre = torch.cat((src_pre1, src_pre2, src_pre3), 0)\n",
    "\n",
    "        \n",
    "    ### Training\n",
    "        optimizer.zero_grad()\n",
    "        tgcnloss = 0\n",
    "                \n",
    "        train_out_item = torch.zeros(src_data.size(0), pre_len, 0)\n",
    "        train_y_item = torch.zeros(src_data.size(0), pre_len, 0)\n",
    "        for i in range((src_num_nodes//tgt_min_num)-1):\n",
    "            adj, speed, pre = get_part_data(i, src_nodes_part, src_adj, src_data, src_pre)\n",
    "            adj = normalized_adj(adj)\n",
    "            adj = adj[:tgt_min_num, :tgt_min_num]\n",
    "            adj = adj.to(device=device,dtype=torch.float32)\n",
    "\n",
    "            X_batch = speed[:,:,:tgt_min_num].permute(1,0,2)\n",
    "            y_batch = pre[:,:,:tgt_min_num]\n",
    "            X_batch = X_batch.to(device=device)\n",
    "            y_batch = y_batch.to(device=device)\n",
    "\n",
    "            h0 = torch.zeros(X_batch.size(1), tgt_min_num, gru_units, dtype=torch.float32).to(device=device)\n",
    "            out = net(adj, X_batch, h0)\n",
    "\n",
    "\n",
    "            train_out_item = torch.cat((train_out_item,out.detach().cpu()),2)\n",
    "            train_y_item = torch.cat((train_y_item,y_batch.detach().cpu()),2)\n",
    "\n",
    "            loss = tgcn_loss(out, y_batch)\n",
    "            tgcnloss = tgcnloss + loss\n",
    "\n",
    "        tgcnloss = tgcnloss / load_batch_size\n",
    "        tgcn_tlt_loss.append(tgcnloss.item())\n",
    "\n",
    "                \n",
    "        train_out_cpu = train_out_item.detach().cpu().numpy().reshape(-1,train_out_item.size(2))\n",
    "        train_y_cpu = train_y_item.detach().cpu().numpy().reshape(-1,train_out_item.size(2))\n",
    "\n",
    "        train_out_cpu = Normalize(train_out_cpu, src_speed.mean, src_speed.std, src_speed.max, src_speed.min, inv_if_norm)\n",
    "        train_y_cpu = Normalize(train_y_cpu, src_speed.mean, src_speed.std, src_speed.max, src_speed.min, inv_if_norm)\n",
    "\n",
    "        train_rmse, train_mae, train_mape, train_acc, train_r2_score, train_var_score = evaluation(train_y_cpu,train_out_cpu)\n",
    "        \n",
    "        # tqdm.write('TRAIN: rmse={0:.4f}, mae={1:.4f}, acc={2:.4f}, r2={3:.4f}, var={4:.4f}'.format(train_rmse, train_mae, train_acc, train_r2_score, train_var_score))\n",
    "        tgcnloss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # total_batch_loss += batch_loss.item()\n",
    "        # total_domain_loss += domain_loss.item()\n",
    "        # total_similar_loss += similar_loss\n",
    "\n",
    "    Train_loss.append(sum(tgcn_tlt_loss) / n_batches)\n",
    "    viz.line([Train_loss[-1]],[epoch],env=environ, win='train_loss', update='append')\n",
    "    # mean_domain_loss.append(total_domain_loss / n_batches)\n",
    "    # mean_similar_loss.append(total_similar_loss / n_batches)\n",
    "    # mean_total_batch_loss.append(total_batch_loss / n_batches)\n",
    "    print('Trainning loss={0:.4f}'.format(Train_loss[-1]))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        tgt_val_speed = CityDataset(city=tgt_city, typo='speed', seq_len=seq_len, pre_len=pre_len, num_days=tgt_num_days, if_norm=if_norm)\n",
    "        tgt_val_loader = DataLoader(tgt_val_speed, batch_size=tgt_val_speed.__len__(), pin_memory=True, num_workers=0, shuffle=True)\n",
    "        for i ,(speed, pre) in enumerate(tgt_val_loader):\n",
    "            tgt_dt = speed.to(device)\n",
    "            tgt_pre = pre.to(device)\n",
    "        \n",
    "        out_item = torch.zeros(tgt_dt.size(0), pre_len, 0)\n",
    "        y_item = torch.zeros(tgt_dt.size(0), pre_len, 0)\n",
    "        \n",
    "        for i in range(sub_graph_num):\n",
    "            adj, speed, pre = get_part_data(i, tgt_nodes_part, tgt_adj, tgt_dt, tgt_pre)\n",
    "            adj = normalized_adj(adj)\n",
    "            adj = adj[:tgt_min_num, :tgt_min_num]\n",
    "            adj = adj.to(device=device,dtype=torch.float32)\n",
    "\n",
    "            X_batch = speed[:,:,:tgt_min_num].permute(1,0,2)\n",
    "            y_batch = pre[:,:,:tgt_min_num]\n",
    "            X_batch = X_batch.to(device=device)\n",
    "            y_batch = y_batch.to(device=device)        \n",
    "\n",
    "            h0 = torch.zeros(X_batch.size(1), tgt_min_num, gru_units, dtype=torch.float32).to(device=device)\n",
    "            out = net(adj, X_batch, h0)\n",
    "\n",
    "            out_item = torch.cat((out_item,out.detach().cpu()),2)\n",
    "            y_item = torch.cat((y_item,y_batch.detach().cpu()),2)\n",
    "\n",
    "            loss = tgcn_loss(out, y_batch)\n",
    "            tgcnloss = tgcnloss + loss\n",
    "            # optimizer.step()\n",
    "\n",
    "        tgcnloss = tgcnloss / (tgt_dt.size(0))\n",
    "        Val_loss.append(tgcnloss)\n",
    "        out_cpu = out_item.detach().cpu().numpy().reshape(-1,out_item.size(2))\n",
    "        y_cpu = y_item.detach().cpu().numpy().reshape(-1,out_item.size(2))\n",
    "\n",
    "        out_cpu = Normalize(out_cpu, tgt_speed.mean, tgt_speed.std, tgt_speed.max, tgt_speed.min, inv_if_norm)\n",
    "        y_cpu = Normalize(y_cpu, tgt_speed.mean, tgt_speed.std, tgt_speed.max, tgt_speed.min, inv_if_norm)\n",
    "\n",
    "        rmse, mae, mape, acc, r2_score, var_score = evaluation(y_cpu,out_cpu)\n",
    "\n",
    "        \n",
    "        # validation_pred = []\n",
    "        validation_maes.append(mae)\n",
    "        validation_acc.append(acc)\n",
    "        validation_mapes.append(mape)\n",
    "        validation_r2.append(r2_score)\n",
    "        validation_var.append(var_score)\n",
    "        validation_rmses.append(rmse)\n",
    "        \n",
    "             \n",
    "        viz.line([Val_loss[-1].cpu().numpy()],[epoch],env=environ, win='val_loss', update='append')\n",
    "        viz.line([rmse],[epoch],env=environ, win='rmses', update='append')\n",
    "        viz.line([mae],[epoch],env=environ, win='maes', update='append')\n",
    "        viz.line([mape],[epoch],env=environ, win='mapes', update='append')\n",
    "        \n",
    "        print('VAL: rmse={0:.4f}, mae={1:.4f}, mape={2:.4f}, acc={3:.4f}, r2_score={4:.4f}, var_score={5:.4f}'.format(rmse, mae, mape, acc, r2_score, var_score))\n",
    "        print('VAL loss={0:.4f}'.format(Val_loss[-1]))\n",
    "\n",
    "\n",
    "#     if not os.path.exists(cwd+'/pkl'):\n",
    "#         os.makedirs(cwd+'/pkl')\n",
    "#     if validation_mapes[-1] < min_mape:\n",
    "#         state = {'TF_model':TranFeatureNet.state_dict(),'D':discriminator.state_dict(), 'net':net.state_dict(), 'min_mape':validation_mape[-1]}\n",
    "#         torch.save(state, cwd+'/pkl/NETS_params.pkl')\n",
    "#         min_mape = validation_mapes[-1]\n",
    "\n",
    "    # tqdm.write(f'EPOCH {epoch:03d}: mean_tgcn_tlt_loss={mean_tgcn_tlt_loss[-1]:.4f}, '\n",
    "            #    f'domain_loss={mean_domain_loss[-1]:.4f}, similar_loss={mean_similar_loss[-1]:.4f}, '\n",
    "            #    f'mean_total_batch_loss={mean_total_batch_loss[-1]:.4f}')   \n",
    "\n",
    "    # tqdm.write(f'EPOCH {epoch:03d}: mean_tgcn_tlt_loss={mean_tgcn_tlt_loss[-1]:.4f}, '\n",
    "            #    f'rmse={rmse:.4f}, mae={mae:.4f}, acc={acc:.4f}, r2={r2_score:.4f}, var={var_score:.4f}')\n",
    "\n",
    "np.savez('losses_400',Train_loss=Train_loss, Val_loss=Val_loss, Val_maes=validation_maes, Val_mape=validation_mapes, Val_acc=validation_acc, \\\n",
    "                Val_rmse=validation_rmses, Val_r2=validation_r2, Val_var=validation_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-color",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
